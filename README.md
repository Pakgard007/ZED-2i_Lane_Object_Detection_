à¹à¸™à¹ˆà¸™à¸­à¸™! à¸”à¹‰à¸²à¸™à¸¥à¹ˆà¸²à¸‡à¸™à¸µà¹‰à¸„à¸·à¸­à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡ **README.md** à¸ à¸²à¸©à¸²à¸­à¸±à¸‡à¸à¸¤à¸©à¹à¸šà¸šà¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸‡à¹ˆà¸²à¸¢à¸ªà¸³à¸«à¸£à¸±à¸šà¹‚à¸›à¸£à¹€à¸ˆà¸à¸•à¹Œ GitHub à¸‚à¸­à¸‡à¸„à¸¸à¸“ à¸‹à¸¶à¹ˆà¸‡à¸›à¸£à¸°à¸à¸­à¸šà¸”à¹‰à¸§à¸¢à¹‚à¸„à¹‰à¸”à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£ **à¸à¸¶à¸à¸à¸™ YOLOP**, **à¸„à¸²à¸¥à¸´à¹€à¸šà¸£à¸•à¸à¸¥à¹‰à¸­à¸‡ ZED 2i**, **à¸£à¸°à¸šà¸šà¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¸ªà¸´à¹ˆà¸‡à¸à¸µà¸”à¸‚à¸§à¸²à¸‡**, à¹à¸¥à¸° **à¸£à¸°à¸šà¸šà¸„à¸§à¸šà¸„à¸¸à¸¡à¸à¸²à¸£à¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™à¸—à¸µà¹ˆà¸‚à¸­à¸‡à¸£à¸–à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´**:

---

# ðŸš— Autonomous Driving System with YOLOP + ZED 2i + ROS 2

This repository contains code for a complete perception and control system for an autonomous vehicle using:

* YOLOP for Drivable Area and Lane Detection
* ZED 2i Stereo Camera for 3D object detection and depth estimation
* ROS 2 for real-time communication and vehicle control
* Optional camera calibration tools and fine-tuning script for YOLOP

---

## ðŸ“ Project Structure

```bash
.
â”œâ”€â”€ calibration/                # Calibrate ZED 2i stereo camera
â”œâ”€â”€ config/                     # YOLOP training configuration
â”œâ”€â”€ interface/                  # Environment perception and object tracking
â”œâ”€â”€ drive_control/             # ROS 2 Node to control vehicle movement
â”œâ”€â”€ weights/                   # Pretrained YOLOP model (e.g. End-to-end.pth)
â””â”€â”€ README.md                  # You're here!
```

---

## ðŸ”§ 1. Camera Calibration (ZED 2i)

Use this script to calibrate your stereo ZED camera and estimate **extrinsic parameters**.

ðŸ“„ `calibration/zed2i_calibration.py`

* Finds checkerboard in stereo images
* Calculates and saves `Rotation` and `Translation` matrices
* Output: `extrinsic_parameters.npz`

---

## ðŸ§  2. YOLOP Fine-Tuning (Optional)

Modify and train YOLOP with custom datasets using the provided config structure.

ðŸ“„ `config/yolop_config.py`

* Dataset paths
* Hyperparameters
* Task selection: Detection / Lane / Drivable Area
* Supports single-task or end-to-end training

> âš ï¸ You must set paths to your own datasets and weights before training.

---

## ðŸ‘ï¸â€ðŸ—¨ï¸ 3. Front Environment Perception

The main interface combines ZED SDK + YOLOP model to detect:

* **Drivable area** (green)
* **Lane lines** (red)
* **People and vehicles** in 3D with distance
* **Decision making**: "go", "slow", "brake"

ðŸ“„ `interface/main_interface.py`

* Publishes `/drive_status` topic (`String`): `"go"`, `"slow"`, or `"brake"`
* Visual output with lane overlay and object tracking

---

## ðŸ•¹ï¸ 4. Drive Control Node (ROS 2)

Receives `/cmd_speed`, `/cmd_angle`, and `/drive_status`, then publishes `/final_cmd_vel` as `geometry_msgs/Twist`.

ðŸ“„ `drive_control/drive_control_node.py`

Behavior:

* Full speed on `"go"`
* Gradual slowdown on `"slow"`
* Immediate stop on `"brake"`

---

## âœ… Requirements

* Python â‰¥ 3.8
* ROS 2 (Humble recommended)
* PyTorch
* ZED SDK (with `pyzed.sl`)
* OpenCV
* YACS

---

## â–¶ï¸ Running the System

1. Start the perception system:

```bash
python3 interface/main_interface.py
```

2. Start the ROS 2 control node:

```bash
ros2 run drive_control drive_control_node
```

> Ensure ZED camera is connected and ROS 2 is sourced before running.

---

## ðŸ“¸ Example Output

* Green: Drivable Area
* Red: Lane Lines
* Text: Drive decision (go / slow / brake)

---
