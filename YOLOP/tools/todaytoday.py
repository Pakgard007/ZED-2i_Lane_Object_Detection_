import argparse
import os, sys
import time
import torch
import cv2
import numpy as np
import torchvision.transforms as transforms
import pyzed.sl as sl
import torch.nn.functional as F

# Import YOLOP utilities
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(BASE_DIR)

from lib.config import cfg
from lib.models import get_net
from lib.core.general import non_max_suppression
from lib.utils import plot_one_box, show_seg_result
from lib.core.general import scale_coords


# Normalize for YOLOP
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
transform = transforms.Compose([transforms.ToTensor(), normalize])

# ----------------- ‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á ZED 2i -----------------
zed = sl.Camera()
init_params = sl.InitParameters()
init_params.camera_resolution = sl.RESOLUTION.HD720  
init_params.camera_fps = 60
init_params.depth_mode = sl.DEPTH_MODE.NONE  

if zed.open(init_params) != sl.ERROR_CODE.SUCCESS:
    print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ!")
    exit()

image = sl.Mat()

# ----------------- ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏≤‡∏•‡∏¥‡πÄ‡∏ö‡∏£‡∏ï‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå -----------------
calib_data = np.load("camera_calibration.npz")
mtxL = calib_data["camera_matrix_L"]
distL = np.array([
    -0.07709319689152208,  # k1
     0.06756180189133752,  # k2
     0.00015006759935512075,  # p1 (tangential distortion)
    -6.006342505065124e-05,  # p2 (tangential distortion)
    -0.028545020615709165  # k3
])

extrinsic_data = np.load("extrinsic_parameters.npz")
R = extrinsic_data["R_left"]
T = extrinsic_data["T_left"]

# ----------------- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Homography Matrix -----------------
src_pts = np.array([
    [300, 700], [1000, 700], [550, 300], [750, 300]   
], dtype=np.float32)

dst_pts = np.array([
    [300, 850], [900, 850], [300, 400], [900, 400]   
], dtype=np.float32)

H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC)

#-------------------Create ROI----------------------
def create_roi(image, roi_width_pixels=300, roi_height_pixels=700):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á ROI ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°‡∏ú‡∏∑‡∏ô‡∏ú‡πâ‡∏≤‡πÅ‡∏ô‡∏ß‡∏ï‡∏±‡πâ‡∏á‡∏ï‡∏£‡∏á‡∏Å‡∏•‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•
    roi_width_pixels: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á ROI ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•
    roi_height_pixels: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á‡∏Ç‡∏≠‡∏á ROI ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•
    """
    height, width = image.shape[:2]

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á ROI
    x_start = (width - roi_width_pixels) // 2
    y_start = (height - roi_height_pixels) // 2

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á ROI
    roi = image[y_start:y_start + roi_height_pixels, x_start:x_start + roi_width_pixels]

    # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö ROI ‡πÉ‡∏ô‡∏†‡∏≤‡∏û Front View (‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô BEV)
    cv2.rectangle(image, (x_start, y_start), (x_start + roi_width_pixels, y_start + roi_height_pixels), (0, 255, 0), 2)

    return roi, (x_start, y_start, roi_width_pixels, roi_height_pixels)

# ----------------- ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• YOLOP -----------------
def load_yolop_model(weights_path, device):
    model = get_net(cfg)
    checkpoint = torch.load(weights_path, map_location=device)
    model.load_state_dict(checkpoint['state_dict'])
    model = model.to(device)
    model.eval()
    print(f"‚úÖ Model Loaded: {weights_path}")
    return model

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = load_yolop_model("/home/mag/satoi/python/YOLOP/weights/End-to-end.pth", device)

# ----------------- ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Drivable Area ‡πÅ‡∏•‡∏∞ Lane -----------------
def detect_obstacles(image, model):
    input_image = transform(image).to(device)
    input_image = input_image.unsqueeze(0)

    with torch.no_grad():
        det_out, da_seg_out, ll_seg_out = model(input_image)

    _, _, height, width = input_image.shape
    da_seg_out = F.interpolate(da_seg_out, size=(height, width), mode='bilinear', align_corners=False)
    ll_seg_out = F.interpolate(ll_seg_out, size=(height, width), mode='bilinear', align_corners=False)

    da_seg_mask = torch.max(da_seg_out, 1)[1].squeeze().cpu().numpy()
    ll_seg_mask = torch.max(ll_seg_out, 1)[1].squeeze().cpu().numpy()

    # ‡πÉ‡∏ä‡πâ non_max_suppression ‡∏Å‡∏±‡∏ö det_out
    det_pred = non_max_suppression(det_out[0], conf_thres=0.25, iou_thres=0.45)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å non_max_suppression
    return da_seg_mask, ll_seg_mask, det_pred


# ----------------- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÉ‡∏ô ROI -----------------
def detect_objects_in_roi(roi, model):
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÉ‡∏ô ROI ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô detect_obstacles ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
    da_seg_mask, ll_seg_mask, det_pred = detect_obstacles(roi, model)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ det_pred ‡πÄ‡∏õ‡πá‡∏ô tensor ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if isinstance(det_pred, torch.Tensor):  # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô tensor
        det_pred = det_pred.cpu().numpy()  # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡∏ó‡∏µ‡πà CPU ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numpy array

    # ‡∏ã‡πâ‡∏≠‡∏ô Bounding Boxes ‡πÉ‡∏ô ROI
    roi_with_boxes = draw_bounding_boxes(roi, det_pred)

    return det_pred, roi_with_boxes

# ----------------- ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà -----------------
def decision_making(object_detected):
    if object_detected:
        return "üö´ STOP: Obstacle Ahead!"  # ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÉ‡∏ô ROI
    else:
        return "‚úÖ SAFE: Continue Moving"  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏


def draw_bounding_boxes(image, detections):
    if detections is not None and len(detections):
        for det in detections:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ det ‡πÄ‡∏õ‡πá‡∏ô numpy array ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if isinstance(det, torch.Tensor):
                det = det.cpu().numpy()  # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô tensor ‡πÉ‡∏´‡πâ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡∏ó‡∏µ‡πà CPU ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numpy array

            det = np.array(det)  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numpy array
            det_tensor = torch.tensor(det[:, :4].astype(float), dtype=torch.float32)

            # ‡πÉ‡∏ä‡πâ scale_coords ‡∏Å‡∏±‡∏ö Tensor
            det_tensor = scale_coords(image.shape[:2], det_tensor, image.shape).round()

            # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô numpy.ndarray
            det[:, :4] = det_tensor.numpy().astype(int)

            for *xyxy, conf, cls in reversed(det):
                label = f'Obj {int(cls)} {conf:.2f}'
                plot_one_box(xyxy, image, label=label, color=(0, 0, 255), line_thickness=2)
    return image


# ----------------- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Overlay Drivable Area & Lane -----------------
def overlay_segmentation(image, mask, color=(0, 255, 0), alpha=0.4):
    overlay = np.zeros_like(image, dtype=np.uint8)
    overlay[mask > 0] = color
    blended = cv2.addWeighted(image, 1 - alpha, overlay, alpha, 0)
    
    # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏Å‡∏ï‡∏¥
    blended = cv2.normalize(blended, None, 0, 255, cv2.NORM_MINMAX)
    
    return blended

# ----------------- ‡πÅ‡∏™‡∏î‡∏á ROI ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô Front View -----------------
def show_roi_in_front_view(image, roi, roi_coords):
    x_start, y_start, roi_width, roi_height = roi_coords
    # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡πÉ‡∏ô Front View
    cv2.rectangle(image, (x_start, y_start), (x_start + roi_width, y_start + roi_height), (0, 255, 0), 2)
    return image
    
# ----------------- ‡πÅ‡∏õ‡∏•‡∏á Front View ‡πÄ‡∏õ‡πá‡∏ô BEV -----------------
def transform_to_bev(image, H):
    bev = cv2.warpPerspective(image, H, (1280, 720))
    return bev

# ----------------- ‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á ZED 2i ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö -----------------
while True:
    if zed.grab() == sl.ERROR_CODE.SUCCESS:
        zed.retrieve_image(image, sl.VIEW.LEFT)
        frame = image.get_data()[:, :, :3]  
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  

        # ‡πÅ‡∏Å‡πâ Distortion
        undistorted_frame = cv2.undistort(frame, mtxL, distL)

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Drivable Area ‡πÅ‡∏•‡∏∞ Lane
        da_seg_mask, ll_seg_mask, det_pred = detect_obstacles(undistorted_frame, model)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á ROI ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏•‡∏≤‡∏á‡∏†‡∏≤‡∏û
        roi, roi_coords = create_roi(undistorted_frame)
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏™‡∏¥‡πà‡∏á‡∏Å‡∏µ‡∏î‡∏Ç‡∏ß‡∏≤‡∏á‡πÉ‡∏ô ROI
        det_pred, roi_with_boxes = detect_objects_in_roi(roi, model)

        # Overlay ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á‡∏ö‡∏ô‡∏†‡∏≤‡∏û
        overlay_da = overlay_segmentation(undistorted_frame, da_seg_mask, (0, 255, 0))
        overlay_ll = overlay_segmentation(overlay_da, ll_seg_mask, (0, 0, 255))
        overlay_ll = draw_bounding_boxes(overlay_ll, det_pred)

        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Bird's Eye View
        bev_image = transform_to_bev(overlay_ll, H)

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÉ‡∏ô ROI ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        object_detected = len(det_pred[0]) > 0 if det_pred else False

        # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏´‡∏¢‡∏∏‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏î‡∏¥‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≠
        decision = decision_making(object_detected)

        # ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ú‡∏•‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡∏°‡∏¥‡∏ô‡∏≠‡∏•
        print(f"‚úÖ Detection Results - Drivable Area: {np.sum(da_seg_mask)}, Lanes: {np.sum(ll_seg_mask)}, Objects: {len(det_pred[0]) if det_pred else 0}")
        print(decision)

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô Front View ‡πÇ‡∏î‡∏¢‡πÅ‡∏™‡∏î‡∏á ROI
        cv2.imshow("Front View - YOLOP", overlay_ll)
        cv2.imshow("ROI", roi)  # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà‡πÉ‡∏ô Front View ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡∏°‡∏¥‡∏ô‡∏≠‡∏•
        print(f"Drivable Area: {np.sum(da_seg_mask)}")
        print(f"Lanes: {np.sum(ll_seg_mask)}")
        print(f"Objects: {len(det_pred[0]) if det_pred else 0}")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• BEV
        large_bev = cv2.resize(bev_image, (1280, 720))
        cv2.imshow("Bird's Eye View", large_bev)

        # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° 'q' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏•‡∏π‡∏õ
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

# ----------------- ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á -----------------
zed.close()
cv2.destroyAllWindows()