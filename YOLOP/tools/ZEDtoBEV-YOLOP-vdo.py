import os, sys
import cv2
import torch
import numpy as np

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(BASE_DIR)

import torchvision.transforms as transforms
from lib.config import cfg
from lib.models import get_net

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£ Normalize ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö YOLOP
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
transform = transforms.Compose([transforms.ToTensor(), normalize])

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ CPU ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
device = torch.device("cpu")

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• YOLOP
model = get_net(cfg)
checkpoint = torch.load("YOLOP/weights/End-to-end.pth", map_location=device)
model.load_state_dict(checkpoint['state_dict'])
model.to(device).eval()
print("‚úÖ Model Loaded Successfully!")

# ‡πÄ‡∏õ‡∏¥‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
video_path = "/home/satoi/final.mp4"
cap = cv2.VideoCapture(video_path)

if not cap.isOpened():
    print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÑ‡∏î‡πâ!")
    exit()

# ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ü‡∏£‡∏°‡πÅ‡∏£‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
ret, frame = cap.read()
if not ret:
    print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ü‡∏£‡∏°‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÑ‡∏î‡πâ!")
    cap.release()
    exit()

frame_height, frame_width, _ = frame.shape
print(f"üìè ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠: {frame_width}x{frame_height}")

# ----------------- ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏û‡∏¥‡∏Å‡∏±‡∏î BEV -----------------
top_view_offset = 100  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡πÅ‡∏ô‡∏ß‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡πâ‡∏°‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ö‡∏ô‡∏•‡∏á‡∏°‡∏≤

src_pts = np.float32([
    [frame_width * 0.15, frame_height * 0.95],  # ‡∏°‡∏∏‡∏°‡∏ã‡πâ‡∏≤‡∏¢‡∏•‡πà‡∏≤‡∏á
    [frame_width * 0.85, frame_height * 0.95],  # ‡∏°‡∏∏‡∏°‡∏Ç‡∏ß‡∏≤‡∏•‡πà‡∏≤‡∏á
    [frame_width * 0.35, frame_height * 0.6],  # ‡∏°‡∏∏‡∏°‡∏ã‡πâ‡∏≤‡∏¢‡∏ö‡∏ô
    [frame_width * 0.65, frame_height * 0.6]   # ‡∏°‡∏∏‡∏°‡∏Ç‡∏ß‡∏≤‡∏ö‡∏ô
])

dst_pts = np.float32([
    [250, 720 + top_view_offset],  # ‡∏ã‡πâ‡∏≤‡∏¢‡∏•‡πà‡∏≤‡∏á
    [1030, 720 + top_view_offset],  # ‡∏Ç‡∏ß‡∏≤‡∏•‡πà‡∏≤‡∏á
    [250, top_view_offset],  # ‡∏ã‡πâ‡∏≤‡∏¢‡∏ö‡∏ô
    [1030, top_view_offset]   # ‡∏Ç‡∏ß‡∏≤‡∏ö‡∏ô
])

# ‡πÉ‡∏ä‡πâ Perspective Transform ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ BEV ‡∏ó‡∏µ‡πà‡∏î‡∏π‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
H = cv2.getPerspectiveTransform(src_pts, dst_pts)

# ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ü‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• YOLOP
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("üìå ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÄ‡∏•‡πà‡∏ô‡∏à‡∏ö‡πÅ‡∏•‡πâ‡∏ß!")
        break

    # ‡πÅ‡∏Å‡πâ Distortion
    undistorted_frame = cv2.undistort(frame, np.eye(3), np.zeros(5))

    # Resize ‡πÉ‡∏´‡πâ YOLOP ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
    frame_resized = cv2.resize(undistorted_frame, (640, 640))

    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Tensor
    img_tensor = transform(frame_resized).unsqueeze(0).to(device).float()

    # ‡∏£‡∏±‡∏ô YOLOP
    with torch.no_grad():
        _, da_seg_out, ll_seg_out = model(img_tensor)

    # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á YOLOP
    ll_seg_mask = torch.nn.functional.interpolate(ll_seg_out, size=(frame_height, frame_width), mode='bilinear', align_corners=False)
    _, ll_seg_mask = torch.max(ll_seg_mask, 1)
    ll_seg_mask = ll_seg_mask.int().squeeze().cpu().numpy()

    da_seg_mask = torch.nn.functional.interpolate(da_seg_out, size=(frame_height, frame_width), mode='bilinear', align_corners=False)
    _, da_seg_mask = torch.max(da_seg_mask, 1)
    da_seg_mask = da_seg_mask.int().squeeze().cpu().numpy()

    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô BEV
    bev_image = cv2.warpPerspective(undistorted_frame, H, (1280, 720))
    bev_lanes = cv2.warpPerspective(ll_seg_mask.astype(np.uint8) * 255, H, (1280, 720))
    bev_drivable_area = cv2.warpPerspective(da_seg_mask.astype(np.uint8) * 255, H, (1280, 720))

    # ‡∏ú‡∏™‡∏° Lane ‡πÅ‡∏•‡∏∞ Drivable Area ‡∏ö‡∏ô BEV
    bev_combined = np.zeros((720, 1280, 3), dtype=np.uint8)
    bev_combined[:, :, 1] = bev_drivable_area  # ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡πÅ‡∏™‡∏î‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏±‡∏ö‡∏Ç‡∏µ‡πà
    bev_combined[:, :, 2] = bev_lanes  # ‡∏™‡∏µ‡πÅ‡∏î‡∏á‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏•‡∏ô‡∏ñ‡∏ô‡∏ô

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    cv2.imshow("Front View - Video", frame)
    cv2.imshow("Bird's Eye View", bev_image)
    cv2.imshow("Lane Segmentation BEV", bev_lanes)
    cv2.imshow("Drivable Area BEV", bev_drivable_area)
    cv2.imshow("Combined BEV", bev_combined)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
cap.release()
cv2.destroyAllWindows()
